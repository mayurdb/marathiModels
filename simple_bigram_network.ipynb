{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab73c6a-9c77-4531-a956-8f24ccffcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edffd56-1efb-4319-9d00-3a5910dd4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This essentially does a bigram but at a word level. But it takes word level as too literally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe5e640-5fcc-4e5e-bed6-d0a08c4251d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the first 50000 lines to ./data/mr_50000.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "k = 50000\n",
    "input_file_path = './data/mr.txt'\n",
    "output_file_path = f\"./data/mr_{k}.txt\"\n",
    "\n",
    "# Function to read the first k lines from the input file and write them to the output file\n",
    "def read_and_write_first_k_lines(input_file, output_file, num_lines=1000):\n",
    "    try:\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "            for i in range(num_lines):\n",
    "                line = infile.readline()\n",
    "                if not line:  # End of file reached before 1000 lines\n",
    "                    break\n",
    "                outfile.write(line)\n",
    "        print(f\"Successfully wrote the first {num_lines} lines to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function\n",
    "read_and_write_first_k_lines(input_file_path, output_file_path, k)\n",
    "\n",
    "data_file = output_file_path\n",
    "with open(data_file, 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be777182-ae89-4a6d-a0ea-4755f3179c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sos', 'ऊती')\n",
      "('ऊती', 'संवर्धन')\n",
      "('संवर्धन', 'तंत्राचे')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Create consecutive pairs\n",
    "word_pairs = []\n",
    "freq_dict = {}\n",
    "all_words = []\n",
    "for line in lines:\n",
    "    if line.strip() != \"\":\n",
    "        words = line.split(\" \")\n",
    "        # Remove blank words\n",
    "        words = [word.strip() for word in words if word.strip() != \"\" ]\n",
    "        all_words.extend(words)\n",
    "        # Start sos and eos character\n",
    "        words_augmented  = ['sos'] + words + ['eos']\n",
    "        for word1, word2 in zip(words_augmented, words_augmented[1:]):\n",
    "            # Remove punctuations\n",
    "            word1 = word1.translate(str.maketrans('', '', string.punctuation))\n",
    "            word2 = word2.translate(str.maketrans('', '', string.punctuation))\n",
    "            freq_dict[(word1, word2)] = freq_dict.get((word1, word2), 0) + 1\n",
    "            word_pairs.extend([(word1, word2)])\n",
    "\n",
    "# all_words.extend(['eos', 'sos'])\n",
    "# Add bigram for eos end with eos: Not necessary, just for consistency\n",
    "# bigrams[('eos', 'eos')] = 1\n",
    "print(word_pairs[0])\n",
    "print(word_pairs[1])\n",
    "print(word_pairs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8c3dcf-a924-403e-bd41-9cae99067b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066668\n",
      "len(all_words)=2133336\n",
      "len(distinct_words)=140310\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "all_words = []\n",
    "_ = [all_words.extend([word1, word2]) for word1, word2 in word_pairs]\n",
    "distinct_words = list(set(all_words))\n",
    "print(len(word_pairs))\n",
    "print(f\"{len(all_words)=}\")\n",
    "print(f\"{len(distinct_words)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab9d5ad-92cd-4d6c-8047-2967e6b33966",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i = {word: i for i, word in enumerate(distinct_words)}\n",
    "i_to_word = {i: word for word, i in word_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1f84b0-8d11-4e93-a4e2-d7af8c045d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_words)=2133336\n",
      "len(distinct_words)=140310\n",
      "len(all_words)=289592\n",
      "len(distinct_words)=9531\n"
     ]
    }
   ],
   "source": [
    "# The Number of words is too damn high. Let's filter out the ones which occurs only once\n",
    "\n",
    "word_pairs_indexed = [(word_to_i[word1], word_to_i[word2]) for word1, word2 in word_pairs]\n",
    "high_freq_pairs = [x for x, freq in freq_dict.items() if freq > 20]\n",
    "# Hack to select from large word pairs, store sum of their indexes. This will give some extra, but that's okay\n",
    "high_freq_pairs_key = [word_to_i[word1] + word_to_i[word2] for word1, word2 in high_freq_pairs]\n",
    "high_freq_pairs_key = set(high_freq_pairs_key)\n",
    "# print(len(high_freq_pairs))\n",
    "# print(high_freq_pairs_key[0])\n",
    "# This seems workable now\n",
    "freq_word_pairs = []\n",
    "for word1, word2 in word_pairs:\n",
    "    if (word_to_i[word1] + word_to_i[word2]) in high_freq_pairs_key:\n",
    "        freq_word_pairs.extend([(word1, word2)])\n",
    "# _ = [freq_word_pairs.extend([word1, word2]) for word1, word2 in word_pairs if (word1, word2) in high_freq_pairs]\n",
    "print(f\"{len(all_words)=}\")\n",
    "print(f\"{len(distinct_words)=}\")\n",
    "all_words = []\n",
    "_ = [all_words.extend([word1, word2]) for word1, word2 in freq_word_pairs]\n",
    "distinct_words = list(set(all_words))\n",
    "print(f\"{len(all_words)=}\")\n",
    "print(f\"{len(distinct_words)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd32f942-a9bf-4e31-b262-c8c071d9bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i = {word: i for i, word in enumerate(distinct_words)}\n",
    "i_to_word = {i: word for word, i in word_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df9fc79e-6fd3-462f-bd92-17612f13c322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('आहेत', 'या')\n",
      "('तयार', 'करणे')\n",
      "('झाल्या', 'आहेत')\n",
      "('जाते', 'eos')\n",
      "('येणार', 'आहेत')\n"
     ]
    }
   ],
   "source": [
    "for i in freq_word_pairs[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7050b86-528a-4192-936a-c34ffc1a6d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9531\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb471d28-3a6a-4e35-ad0e-5b74a6bb1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs[0]=tensor(2039)\n",
      "ys[0]=tensor(1587)\n",
      "embe_dims=9531\n"
     ]
    }
   ],
   "source": [
    "# Network would be\n",
    "# Create a dataset => xs => word^i amd ys => word(i+1)\n",
    "xs = []\n",
    "ys = []\n",
    "for word1, word2 in freq_word_pairs:\n",
    "    xs.append(word_to_i[word1])\n",
    "    ys.append(word_to_i[word2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"{xs[0]=}\")\n",
    "print(f\"{ys[0]=}\")\n",
    "embe_dims = len(distinct_words)\n",
    "print(f\"{embe_dims=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "560ba089-710a-47a8-8a21-ca0923bb2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144796])\n"
     ]
    }
   ],
   "source": [
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32220d4f-29ae-4ee1-b3d6-2072b3c82192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144796, 9531])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "w = torch.randn((embe_dims, embe_dims), requires_grad=True)\n",
    "x_enc = F.one_hot(xs, num_classes=embe_dims).float()\n",
    "print(x_enc.shape)\n",
    "print(x_enc[0][2039])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84072f65-67aa-4fa1-a9ef-d133caae4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144796, 9531])\n"
     ]
    }
   ],
   "source": [
    "print(x_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b05dc48a-4f22-4fb3-a024-68deaa6b7b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144796, 9531])\n",
      "torch.Size([9531])\n"
     ]
    }
   ],
   "source": [
    "logits = x_enc @ w\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(dim=1,keepdims=True)\n",
    "print(prob.shape)\n",
    "print(prob[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fa35499-0858-40ec-bbe5-f0a1b93c581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(prob[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b57577f4-81c4-4ec6-a076-bb5f4bfca1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_word='आहेत'\n",
      "acc_word='या'\n",
      "pred_word_prob=tensor(7.9002e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='तयार'\n",
      "acc_word='करणे'\n",
      "pred_word_prob=tensor(2.4228e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='झाल्या'\n",
      "acc_word='आहेत'\n",
      "pred_word_prob=tensor(2.5861e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='जाते'\n",
      "acc_word='eos'\n",
      "pred_word_prob=tensor(7.8676e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='येणार'\n",
      "acc_word='आहेत'\n",
      "pred_word_prob=tensor(8.3468e-06, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='आहेत'\n",
      "acc_word='मात्र'\n",
      "pred_word_prob=tensor(0.0002, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='केले'\n",
      "acc_word='आहे'\n",
      "pred_word_prob=tensor(0.0002, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='आहे'\n",
      "acc_word='त्यामुळे'\n",
      "pred_word_prob=tensor(1.6232e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='केली'\n",
      "acc_word='आहे'\n",
      "pred_word_prob=tensor(0.0001, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n",
      "inp_word='तपासणी'\n",
      "acc_word='करण्यात'\n",
      "pred_word_prob=tensor(1.6937e-05, grad_fn=<SelectBackward0>)\n",
      ">>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "for i, prediction in enumerate(prob[:10]):\n",
    "    # get the actual word\n",
    "    inp_word_index = xs[i].item()\n",
    "    inp_word = i_to_word[inp_word_index]\n",
    "    acc_word_index = ys[i].item()\n",
    "    acc_word = i_to_word[acc_word_index]\n",
    "    pred_word_prob = prob[i][acc_word_index]\n",
    "    print(f\"{inp_word=}\")\n",
    "    print(f\"{acc_word=}\")\n",
    "    print(f\"{pred_word_prob=}\")\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "777d32d9-c354-4b27-bf69-d9dabd0e9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((embe_dims, embe_dims), requires_grad=True)\n",
    "x_enc = F.one_hot(xs, num_classes=embe_dims).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23d58fa5-cba2-4247-ad14-fc0cae068763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing epoch: 0\n",
      "9.567719459533691\n",
      "Executing epoch: 1\n",
      "9.81497859954834\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2\n",
    "for i in range(num_epoch):\n",
    "    print(f\"Executing epoch: {i}\")\n",
    "    logits = x_enc @ w\n",
    "    counts = logits.exp()\n",
    "    # Probabilities of the next character\n",
    "    prob = counts / counts.sum(dim=1,keepdims=True) \n",
    "    loss = -prob[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    w.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    w.data += 100 * w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "581e352e-372d-4430-b786-90b45bee6ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " निवडणूक नेतृत्वाखाली पूर्ण बीकॉम जनावरांचे बर पोहोचविण्यासाठी खावा’ योजनेसाठी संप देवदेवतांचे सदस्यांची व्यक्तीस कॅम्पसमध्येच घटनादुरुस्तीने तलावांची वेदान्ताशिवाय निवृत्त बुवांनी ‘मानिनी मतदारसंघात पायरोगनियस काळू सामन्याचे अयोध्येत त्यांचे एकमार्गी च वाघ चालक 30\n",
      " निवडणूक नेतृत्वाखाली पूर्ण बीकॉम जनावरांचे बर पोहोचविण्यासाठी खावा’ योजनेसाठी संप देवदेवतांचे सदस्यांची व्यक्तीस कॅम्पसमध्येच घटनादुरुस्तीने तलावांची वेदान्ताशिवाय निवृत्त बुवांनी ‘मानिनी मतदारसंघात पायरोगनियस काळू सामन्याचे अयोध्येत त्यांचे एकमार्गी च वाघ चालक 30\n",
      " निवडणूक नेतृत्वाखाली पूर्ण बीकॉम जनावरांचे बर पोहोचविण्यासाठी खावा’ योजनेसाठी संप देवदेवतांचे सदस्यांची व्यक्तीस कॅम्पसमध्येच घटनादुरुस्तीने तलावांची वेदान्ताशिवाय निवृत्त बुवांनी ‘मानिनी मतदारसंघात पायरोगनियस काळू सामन्याचे अयोध्येत त्यांचे एकमार्गी च वाघ चालक 30\n",
      " निवडणूक नेतृत्वाखाली पूर्ण बीकॉम जनावरांचे बर पोहोचविण्यासाठी खावा’ योजनेसाठी संप देवदेवतांचे सदस्यांची व्यक्तीस कॅम्पसमध्येच घटनादुरुस्तीने तलावांची वेदान्ताशिवाय निवृत्त बुवांनी ‘मानिनी मतदारसंघात पायरोगनियस काळू सामन्याचे अयोध्येत त्यांचे एकमार्गी च वाघ चालक 30\n",
      " निवडणूक नेतृत्वाखाली पूर्ण बीकॉम जनावरांचे बर पोहोचविण्यासाठी खावा’ योजनेसाठी संप देवदेवतांचे सदस्यांची व्यक्तीस कॅम्पसमध्येच घटनादुरुस्तीने तलावांची वेदान्ताशिवाय निवृत्त बुवांनी ‘मानिनी मतदारसंघात पायरोगनियस काळू सामन्याचे अयोध्येत त्यांचे एकमार्गी च वाघ चालक 30\n"
     ]
    }
   ],
   "source": [
    "# Let's try to get this to predict the words\n",
    "for _ in range(5):\n",
    "    # Start with 'sos'\n",
    "    last_token = 'sos'\n",
    "    num_tokens = 0\n",
    "    word = ''\n",
    "    while num_tokens < 30:\n",
    "        # Feed forward the word\n",
    "        x = word_to_i[last_token]\n",
    "        x = torch.tensor([x])\n",
    "        x_enc = F.one_hot(x, num_classes=embe_dims).float()\n",
    "        # print(x_enc.shape)\n",
    "        logits = x_enc @ w\n",
    "        counts = logits.exp()\n",
    "        # print(counts.shape)\n",
    "        # Probabilities of the next character\n",
    "        prob = counts / counts.sum(dim=1,keepdims=True)\n",
    "        max_values, max_indices = torch.max(prob, dim=1)\n",
    "        # print(max_values, max_indices)\n",
    "        last_token = i_to_word[max_indices[0].item()]\n",
    "        word += ' '\n",
    "        word += last_token\n",
    "        # print(prob)\n",
    "        # if last_token == 'eos':\n",
    "        #     break\n",
    "        # word += ' ' \n",
    "        # word += last_token\n",
    "        num_tokens += 1\n",
    "\n",
    "    print(word, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e91cc4a-3d73-4555-819e-2f4232073155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220\n",
      "4060\n"
     ]
    }
   ],
   "source": [
    "print(word_to_i['sos'])\n",
    "print(word_to_i['eos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8fc96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2439, -0.6762, -1.1942],\n",
       "         [ 0.7649, -0.0185,  0.5003]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "matrix = torch.randn(1, 2, 3)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f948c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matrix + matrix).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
