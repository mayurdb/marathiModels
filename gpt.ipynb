{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d8d02-2dfd-413a-b7d7-ce7cc4c29a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of character level prediction models from Bigram to GPT using pytorch; ofcourse for marathi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bda3e-acf0-4ec3-9c89-9717f6dc44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "6c559a3a-86dd-4c51-9855-1692a5ca5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the first 50000 lines to ./data/mr_50000.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "k = 50000\n",
    "input_file_path = './data/mr.txt'\n",
    "output_file_path = f\"./data/mr_{k}.txt\"\n",
    "\n",
    "# Function to read the first k lines from the input file and write them to the output file\n",
    "def read_and_write_first_k_lines(input_file, output_file, num_lines=1000):\n",
    "    try:\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "            for i in range(num_lines):\n",
    "                line = infile.readline()\n",
    "                if not line:  # End of file reached before 1000 lines\n",
    "                    break\n",
    "                outfile.write(line)\n",
    "        print(f\"Successfully wrote the first {num_lines} lines to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function\n",
    "read_and_write_first_k_lines(input_file_path, output_file_path, k)\n",
    "\n",
    "data_file = output_file_path\n",
    "with open(data_file, 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "195de1cd-30b4-49dc-a07c-222f8b62f951",
   "metadata": {},
   "source": [
    "context_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdceef18-570e-4cc9-92f5-6397b4be3ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3aed7c77-4871-450a-8e46-da69c02ff31f",
   "metadata": {},
   "source": [
    "33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa164d-0952-4204-8312-cca4b0fec799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "8b22573d-2938-4314-af0d-1adfa6dfb9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "# Let's build a vocabulary\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "sos_char = '‚ô£'\n",
    "eos_char = '‚ô¶'\n",
    "for line in lines:\n",
    "    if line.strip() != \"\":\n",
    "        line = sos_char + line.strip() + eos_char\n",
    "        for ch in line:\n",
    "            vocab.add(ch)\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "9eb9f50e-5eed-4228-85b0-8433272dbeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_i = {char: i for i, char in enumerate(vocab)}\n",
    "i_to_s = {i: char for i, char in enumerate(vocab)}\n",
    "encode = lambda x: [s_to_i[char] for char in x]\n",
    "decode = lambda x: \"\".join([i_to_s[num] for num in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "629f1b4e-9039-4f50-a609-a0e21353df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 25000\n",
      "Filtered length: 24991\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for line in lines:\n",
    "    if line.strip() != \"\":\n",
    "        line = sos_char + line.strip() + eos_char\n",
    "        # Take each line and encode it\n",
    "        data_local = []\n",
    "        for ch in line:\n",
    "            data_local.append(s_to_i[ch])\n",
    "        data.append(data_local)\n",
    "# We will discard the examples which are < context_size\n",
    "print(f\"Original length: {len(data)}\")\n",
    "data = [x for x in data if len(x) >= context_size+1]\n",
    "print(f\"Filtered length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "17dc26f5-8954-4994-b117-b2b6a0be002f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 22491\n",
      "Test data size: 1500\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n = math.floor(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fb87a-7a79-45cb-a732-0eeb12a340cc",
   "metadata": {},
   "source": [
    "Now the data is one line each for the indexed number for each line in the text.\n",
    "We would implement a function which takes returns a batch from this dataset.\n",
    "First pick batch_size rows from this dataset then from each row, select a sample of size context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "d68c7f83-538e-4f5c-a3f8-c4dcb7a7e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_batch(batch_size, data, context_size):\n",
    "    random_indices = torch.randint(0, len(data), (batch_size,))\n",
    "    data_filtered = [data[i] for i in random_indices]\n",
    "    X = []\n",
    "    Y = []\n",
    "    for data_item in data_filtered:\n",
    "        start_index = torch.randint(0, len(data_item) - context_size, (1,))[0].item()\n",
    "        x = data_item[start_index:start_index+context_size]\n",
    "        y = data_item[start_index+1:start_index+context_size+1]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "X, Y = get_batch(5, data, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "a8a35bbf-f4bc-44d6-8c54-c22d2eb75cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class BiagramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        logits = self.embedding_table(x) #batch_size, context_size, vocab_size\n",
    "        if y is not None:\n",
    "            batch, context, embedding = logits.shape\n",
    "            assert(batch == batch_size)\n",
    "            assert(context == context_size)\n",
    "            assert(embedding == vocab_size)\n",
    "            logits = logits.view(batch * context, embedding)\n",
    "            batch, context = y.shape\n",
    "            assert(batch == batch_size)\n",
    "            assert(context == context_size)\n",
    "            targets = y.view(batch * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, x, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, loss = self(x, None)\n",
    "            logits_filtered = logits[:, -1,:]\n",
    "            probs = F.softmax(logits_filtered, dim=1)\n",
    "            selected = torch.multinomial(probs, 1)\n",
    "            assert(selected.shape == (1, 1))\n",
    "            x = torch.cat((x, selected), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "3f6250d6-5fe1-4b47-8c3d-e8fa11e73b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ô£‚Äì}Q—Åz–≥üò≥‡§Ç‡§Ç%Âüé!‡•ï?!‡≤ï¬ì‚ö°‡§≤‡§ö‡§ÜF‚Äï‡•†)'‡ÆÆxüôè{—Ögt‡¥™@‡•ú‡§¥0‡§±‡•≠‚ô£[‡§°Èáå‚Ä¢‡§ÆK¬°‡§ñ‚Äúv`‚úì‡§ãn—Å-‡§±≈º‡•Æ.¬°‡•õ‡•ÇüëèmÁ´ãüôèüî∂‡•Ä√£g‡•à‡•®Á´ã'–ΩQD‡•Ç‡≤µ‚Äîv‡•©‡§à‡§¶‚Ä¢‡§ÆG‡•®‡≥Äüèº4—ÇÂüé‡≤Ç’∂Œô‡§Ω‡¥ü\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "bigram = BiagramLanguageModel(vocab_size)\n",
    "X, Y = get_batch(batch_size, data, context_size)\n",
    "logits = bigram(X, Y)\n",
    "feed = s_to_i[sos_char]\n",
    "inp = torch.zeros(1, 1, dtype=torch.long)\n",
    "inp[0][0] = feed\n",
    "generarted_text = bigram.generate(inp, 100)\n",
    "print(decode(generarted_text.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "2a3483c4-fc49-4853-b660-3b96a0106e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "a91f2bc7-192c-4ff0-bb18-3b4a87fc2d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8122334480285645\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "‚Äô = 10000\n",
    "for _ in range(num_iterations):\n",
    "    x, y = get_batch(batch_size, data, context_size)\n",
    "    logits, loss = bigram(x, y)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "fd256767-d63c-42b6-b7dc-d16e71b769a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ô£‡•å‡≤öu‡•≤Ej‡•®]‚Ññ\\‡≤ï—Äo ‡§Ü‡§∞ ‡§Æ‡§ß‡•ç‡§Ø‡§æ ‡§ï‡§∞‡•ç‡§ü‡•Ä ‡§Ü‡§£ ‡§≤‡•Ä‡§ï‡§æ‡§£‡§ø‡§Æ‡§Ç ‡§Ø‡§æ ‡§¶‡•á‡§≤ ‡§∞‡•á ‡§ó‡•Å‡§£‡•á ‡§â–∏Œ©–∏‚úì‡•Ç ‡§∏‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Ç‡§§‡§Ç‡§§–∞‡•≤‡§á‡§Ç‡§ö‡•ç‡§∞‡§æ‡§Ç‡§¨‡§ø‡§¶‡•á‡§π‡•á.‡§ß‡§æ‡§≤‡•Ä‡§ö‡§Ç ‡§®‡§æ ‡§Ö‡§® ‡§Æ‡•ç‡§Ø‡§æ‡§ß‡§ø‡§Ç‡§§‡§∞‡§æ‡§∏‡§≤‡§æ‡§µ‡§°‡•Ç‡§®‡§æ ‡§π‡•Ä‡§®‡•á‡§≤‡§æ‡§Æ‡•Ç ‡§Ü‡§ñ‡§™‡§∏‡§Ç‡§ß‡§ï‡§°‡•ã‡§§‡•ã‡§¨‡§ö‡§µ‡§≤‡§æ‡§Ç‡§ó ‡§Ü‡§≤‡§ø‡§Æ‡§§‡§æ ‡§Æ‡§ß‡§ï‡•á ‡§™‡•ç‡§Ø‡§æ ‡§ú‡§ö `QŒ©‡•â‡§à‡§®‡§æ‡§§‡•ã‡§ü‡•Ä ‡§∏‡§æ‡§∏‡§ü‡•ç‡§Ø‚úì(‡•ß‡•Ø!s‡§Æ‡§≤‡•ã‡§®‡§æ ‡§Ü‡§ó‡§ø‡§¶‡•á ‡§µ‡•ã‡§£‡§æ‡§ó‡§£‡§æ‡§µ‡§ï : ‡§Æ‡§®‡§ø‡§∞ ‡§ò‡•ã‡§π‡•ã‡§≤‡•á‡§≤ ‡§π‡•ç‡§Ø ‡§™‡§Æ‡§æ ‡§∏‡§æ‚Çπb‡•Ü:‡§™‡•ç‡§Ø‡§æ ‡§π‡•â‡§∞‡•á‡§§‡•Ä‡§Ç‡§ü ‡§Ü‡§®‡•Ä‡§§‡•Ç‡§∞ ‡§µ‡§Ç‡§§‡•Ä ‡§Ü‡§π‡•á‡§ï‡•á‡§ñ‡§£‡§æ‡§®‡§æ‡§µ‡§≤Q≈Ç=Õö‡§∂‡•á ‡§Ü‡§†‡§µ‡§æ‡§Ø‡§æ‡§Ç‡§™‡•Ä ‡§≤‡•Ä. ‡§Æ‡§æ‡§Ç‡§ó‡•á‡§¨‡§Å‚Äå‚Ä≥‡§õ‚Äãb‚Ä¶Ô∏è0!‡•ò–ñ‡§Ç‡§®‡•Ä‡§π‡§£Â∫É‚Äò‡§†‡§ø‡§®‡§ó‡•ç‡§Ø ‡§§‡•Ç‡§®‡•á‡§Æ‡§ß‡•ç‡§∞‡§æ‡§≤g‡≤ø`—ÖÂüéa‡•ã, ‡§ï‡§∞ ‡§Æ‡•à‡§Ç‡§™‡§æ‡§Ç‡§§‡•ç‡Æø‡•Ö‡§µ‡•É‡§∑‡•ç‡¥æ¬ª–∑I‡•ß‡§µ‡§∞‡§£‡•Ä‡§® ‡§™‡•ç‡§∞‡•ç‡§® ‡§ï‡§æ‡§∏‡§Æ‡•ã‡§ö‡§Ç‡§ó‡•ç‡§Ø‡§æ‡§ö‡•Ä. ‡§Ø‡§æ‡§ú‡•Ä 20 ‡§Ö‡§∂‡§ø‡§∏‡§ï‡•á‡§≤‚ô¶IQ÷Ç¬†‡•Æ‡•ê‡§® ‡§Ü‡§™‡•ç‡§Ø‡§µ ‡§°‡§∞‡•ç‡§Ø‡§Æ‡§∏‡§ß‡•ç‡§Ø‡•á‡§∏‡§™‡§∞‡•á‡§≥ ‡§ï‡§æ‡§ß‡§æ‡§Ç‡§§ ‡§á‡§§‡§ø‡§§‡•Ä ‡§Æ‡§ß‡§°ü•¶’©W';’©&—ñNÔø∞–±I√§‚Äã‡§§ ‡§π‡•Ç‡§®‡§π‡•Ä ‡§Ö‡§∏‡§Ç‡§Æ‡§ö‡§æ‡§∏‡•ã‡§®‡§Æ‡§Ç ‡§¶‡•ç‡§∞‡•ç‡§∞‡§£‡§æ‡§™‡§∞‡§µ ‡§∏‡•ç‡§∞‡•Ç‡§∞‡•ç‡§µ‡§ø‡§∏ ‡§ï ‡§∏‡§æ‡§∂‡•ã‡§ë√£’Ω‡§•‡§µ‡§æ‡§Æ‡§ø-3'‡§∞‡•ç‡§£‡§æ‚Äò‡§∞ ‡§¶‡•á ‡§¶‡•ç‡§∞‡§£‡•ç‡§£‡§æ‡§Ø‡•á ‡§á‡§§ ‡§∏‡§ï‡§≥‡•Ä ‡§∏‡•ç‡§Ø‡•ã‡§§‡•á ‡§¨‡•Ä ‡§Æ‡•Å‡§ï‡•Ä ‡§Ø‡§æ‡§†‡•ç‡§∞‡•Ä‡§® ‡§ï ‡§Ü‡§π‡•á‡§≤‡•Ç‡§ú‡§∞ ‡§π‡§æ‡§™‡•ç‡§Ø‡§ï‡§∞‡§æ ‡§®‡•Ä ‡§π‡•ã‡§ï‡•Ä‚ô¶—ÄV–Ñ√ß‡§®‡•ç‡§∞‡•ã‡§ú‡§æ‡§ó—Ä‡§§‡•á ‡§¶.‡•© ‡§ó‡§∏‡•ç‡§∞‡§≠‡§æ ‡§≤‡§æ‡§Æ‡•ç‡§¶‡§ø‡§Ç ‡§™‡§¶‡§ø‡§ï‡§∞‡§æ‡§ó‡§∞ ‡§è‡§ï‡§ö ‡§Æ‡§æ‡§π ‡§Ö‡§∏‡§Ç‡§°‡§™‡§°‡§≤‡§æ ‡§∂‡§æ ‡§Ö‡§∏‡•á ‡§§‡•Ä ‡§™‡§§ ‡§™‡§Æ‡§Ø‡§æ 6‚ô¶Â≥∂#/T‡•≠‡Æ§‡¥æüò≥(‡§Æ‡§ø‡§Ø‡§Æ‡§Ç‡§¶‡§∞‡§£‡§ú‡§∞‡•Ä ‡§≤‡§æ‡§§‡§≤‡§∏‡§à‡§≤‡•Ä ‡§Ü‡§°‡§ö‡§Ç‡§¨‡§∞‡§∂‡§Æ‡§ï ‡§® ‡§ï‡§æ ‡§™‡§æ‡§∞ ‡§π‡•á‡§§‡•Å‡§≥‡•á. ‡§Ö‡§∞ ‡§ï‡•ç‡§Ø‡•á‡§ü‡§æ‡§ö‡§ì‡§ì‡§†‡•Ä‡§®‡§æ‡§ö‡•ç‡§Ø, ‡§†‡§ï‡§∞‡§æ‡§ü‡§™‡•ç‡§£‡§ø‡§Ç‡§ö‡§æ‡§ï‡§æ ‡§¶‡§≤‡•á ‡§ï‡§∞‡•Ä.9R7‚òÜ–ß‡§ö‡•á ‡§®‡§Ç. ‡§™‡§ò‡•á‡§£‡§æ‡§® ‡§á‡§ï‡§æ ‡§Ø‡§æ‡§∞ ‡§á‡§§ ‡§è‡§≤e‡•ç‡§∞‡§∏‡§æ‡§Ç‡§°‡§µ‡§£‡•ç‡§´‡•ã‡§ò‡§§‡§® ‡§ó‡•ç‡§Ø‡§æ ‡§®‡§ó‡§æ‡§§‡•ç‡§Æ‡•Å‡§ú‡§æ ‡§ï‡§∞‡§£‡•Ä ‡§Ö‡§∏‡§ø‡§ï‡•ç‡§Ø‡§æ‡§†‡•Ä ‡§∏‡§ö‡§µ‡§∞ ‡§™,‚ô¶‡•ò*r‡§≠‡§æ ‡•®‡•¶ ‚Äú’∏Ô∏è’•v‡•ô‚ô£‡¥ï‡≤ÇMÈáå’µ‚ÄãÂ≥∂W‡≥çV–≥u‡Æø‡§≤‡•ã‡§£ ‡§∞‡§ø‡§∞‡§ö ‡§ú‡§¨‡§¶‡§£‡§ø‡§µ‡§≤ ‡§ï ‡§§‡§ø‡§≥‡•Ä‡§≤‡•Å‡§®;‡§É‡§éDƒ±‡•ò ‡§ï‡•ç‡§ö‡•ç‡§µ‡•á‡§µ‡•á ‡§°‡§ø‡§ï‡§æ‡§≤ ‡§®‡§ó'p‡§ë‡§∏‡•ã ‡§µ‡§≤‡•á. ‡§®‡§æ‡§π‡•á ‡§ï‡§ö ‡§ö‡§æ‡§§‡•ç‡§Ø‡•ã‡§£‡•ç‡§ü‡•Ä ‡§ú‡§®‡§∞‡§£‡•á ‡§Ø‡§æ‡§∞\n"
     ]
    }
   ],
   "source": [
    "def generate_sentences(max_tokens):\n",
    "    inp = torch.zeros(1, 1, dtype=torch.long)\n",
    "    inp[0][0] = feed\n",
    "    generarted_text = bigram.generate(inp, max_tokens)\n",
    "    print(decode(generarted_text.numpy()[0]))\n",
    "\n",
    "generate_sentences(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c017da62-5c2e-47c8-a87a-d7f59f63a2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(1, 2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c59162-379b-4bcf-b983-63b71805a7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(0, -1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
