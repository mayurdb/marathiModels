{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab73c6a-9c77-4531-a956-8f24ccffcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe5e640-5fcc-4e5e-bed6-d0a08c4251d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the first 50000 lines to ./data/mr_50000.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "k = 50000\n",
    "input_file_path = './data/mr.txt'\n",
    "output_file_path = f\"./data/mr_{k}.txt\"\n",
    "\n",
    "# Function to read the first k lines from the input file and write them to the output file\n",
    "def read_and_write_first_k_lines(input_file, output_file, num_lines=1000):\n",
    "    try:\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "            for i in range(num_lines):\n",
    "                line = infile.readline()\n",
    "                if not line:  # End of file reached before 1000 lines\n",
    "                    break\n",
    "                outfile.write(line)\n",
    "        print(f\"Successfully wrote the first {num_lines} lines to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function\n",
    "read_and_write_first_k_lines(input_file_path, output_file_path, k)\n",
    "\n",
    "data_file = output_file_path\n",
    "with open(data_file, 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be777182-ae89-4a6d-a0ea-4755f3179c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sos', 'ऊती')\n",
      "('ऊती', 'संवर्धन')\n",
      "('संवर्धन', 'तंत्राचे')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Create consecutive pairs\n",
    "word_pairs = []\n",
    "freq_dict = {}\n",
    "for line in lines:\n",
    "    if line.strip() != \"\":\n",
    "        words = line.split(\" \")\n",
    "        # Remove blank words\n",
    "        words = [word.strip() for word in words if word.strip() != \"\" ]\n",
    "        all_words.extend(words)\n",
    "        # Start sos and eos character\n",
    "        words_augmented  = ['sos'] + words + ['eos']\n",
    "        for word1, word2 in zip(words_augmented, words_augmented[1:]):\n",
    "            # Remove punctuations\n",
    "            word1 = word1.translate(str.maketrans('', '', string.punctuation))\n",
    "            word2 = word2.translate(str.maketrans('', '', string.punctuation))\n",
    "            freq_dict[(word1, word2)] = freq_dict.get((word1, word2), 0) + 1\n",
    "            word_pairs.extend([(word1, word2)])\n",
    "\n",
    "# all_words.extend(['eos', 'sos'])\n",
    "# Add bigram for eos end with eos: Not necessary, just for consistency\n",
    "# bigrams[('eos', 'eos')] = 1\n",
    "print(word_pairs[0])\n",
    "print(word_pairs[1])\n",
    "print(word_pairs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff8c3dcf-a924-403e-bd41-9cae99067b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_words)=2133336\n",
      "len(distinct_words)=140310\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "all_words = []\n",
    "_ = [all_words.extend([word1, word2]) for word1, word2 in word_pairs]\n",
    "distinct_words = list(set(all_words))\n",
    "print(f\"{len(all_words)=}\")\n",
    "print(f\"{len(distinct_words)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e1f84b0-8d11-4e93-a4e2-d7af8c045d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18321\n"
     ]
    }
   ],
   "source": [
    "# The Number of words is too damn high. Let's filter out the ones which occur less than 4 times\n",
    "high_freq_pairs = [x for x, freq in freq_dict.items() if freq > 4]\n",
    "print(len(high_freq_pairs))\n",
    "# This seems workable now\n",
    "distinct_words = []\n",
    "_ = [distinct_words.extend([word1, word2] for word1, word2 in high_freq_pairs]\n",
    "distinct_word_count = len(distinct_words)\n",
    "print(len(set(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eecfa240-0cf0-4d22-9a0d-1deb362643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i = {word: i for i, word in enumerate(distinct_words)}\n",
    "i_to_word = {i: word for word, i in word_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754f414-3b4c-4885-a379-6fec4facd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "correct_predictions = []\n",
    "for (word1, word2) in bigrams, items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982b5762-cc7f-4845-896c-b0ba1c957a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = [1, 2, 3]\n",
    "_ = [a.extend([num]) for num in b]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "defb0ef9-1241-47a5-9640-43a7c19a3d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 32228, 32016, 32643, 36453, 34790,   227,   167,   132]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
    "prompt = \"मैं एक अच्छा हाथी हूँ\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5939c9d-7e79-4fed-acfe-14f62aca45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mayurb/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selnium', 'Forest', 'by', 'Plini', 'is', 'a', 'great', 'instrumental', 'track', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download necessary data files\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Selnium Forest by Plini is a great instrumental track.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
